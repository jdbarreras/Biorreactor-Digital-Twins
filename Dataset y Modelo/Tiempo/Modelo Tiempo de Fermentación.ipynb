{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Gradient Boosting"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"C9x0S8yV3Fjq1ofqfXbboU",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df =pd.read_csv(\"sin_outliers_rounded.csv\")\n",
    "# Total de filas en el DataFrame\n",
    "n = len(df)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"im15gOHHktZNUA2oy1r8mU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# División del conjunto de datos en 80% entrenamiento y 20% prueba\n",
    "np.random.seed(622)\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, random_state=622)\n",
    "\n",
    "# Convertir columnas de texto a tipo 'category'\n",
    "fct_df_train = df_train.copy()\n",
    "fct_df_test = df_test.copy()\n",
    "\n",
    "for col in fct_df_train.select_dtypes(include=\"object\").columns:\n",
    "    fct_df_train[col] = fct_df_train[col].astype(\"category\")\n",
    "    fct_df_test[col] = fct_df_test[col].astype(\"category\")\n",
    "\n",
    "# Definir variables independientes (X) y dependiente (y)\n",
    "features = [\"Variedad\", \"Altura\", \"Cantidad (L)\", \"Temperatura\", \"pH\", \"Puntaje en taza\"]\n",
    "target = \"Tiempo de fermentación\"\n",
    "\n",
    "# Convertir variables categóricas a variables dummy para el modelo\n",
    "X_train = pd.get_dummies(fct_df_train[features], drop_first=True)\n",
    "X_test = pd.get_dummies(fct_df_test[features], drop_first=True)\n",
    "y_train = fct_df_train[target]\n",
    "y_test = fct_df_test[target]\n",
    "\n",
    "# Asegurar que las columnas de test coincidan con las de train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"mc1zfp9Zn6mmkf3JgfUj5W",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Reproducibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# Concatenar los datasets de entrenamiento y prueba como en R\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "# Proporción de entrenamiento\n",
    "train_fraction = len(X_train) \/ len(X_full)\n",
    "\n",
    "# Entrenar el modelo Gradient Boosting con 10,000 árboles\n",
    "gbm_model = GradientBoostingRegressor(\n",
    "    loss=\"squared_error\", \n",
    "    learning_rate=0.01, \n",
    "    n_estimators=10000, \n",
    "    max_depth=4, \n",
    "    min_samples_leaf=20, \n",
    "    subsample=0.5,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Entrenar con los datos completos\n",
    "gbm_model.fit(X_full, y_full)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": gbm_model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "# Calcular error en cada iteración del modelo\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbm_model.staged_predict(X_test)]\n",
    "best_iter = np.argmin(errors) + 1  # Encontrar la mejor cantidad de árboles\n",
    "\n",
    "print(f\"Best iteration based on test data: {best_iter}\")\n",
    "\n",
    "# Generar predicciones usando la mejor cantidad de árboles\n",
    "gbm_prediction = list(gbm_model.staged_predict(X_test))[best_iter - 1]\n",
    "\n",
    "# Calcular RMSE\n",
    "rmse_gbm = np.sqrt(mean_squared_error(y_test, gbm_prediction))\n",
    "\n",
    "# Calcular R²\n",
    "rsq_gbm = np.corrcoef(y_test, gbm_prediction)[0, 1] ** 2\n",
    "\n",
    "print(f\"The GBM model is on average {round(rmse_gbm, 2)} total points off when predicting new values. \\nIts R squared value is {round(rsq_gbm, 2)}.\")\n",
    "\n",
    "# Graficar valores reales vs predichos\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, gbm_prediction, alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Gradient Boosted Tree Model: Actual vs Predicted Values\")\n",
    "plt.show()"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YMHAUwupEqDr9vh4cDiaAb",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Tiempo < 24 H"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"bsTb5a3ykeho8j3nYm64ae",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"sin_outliers_rounded.csv\")\n",
    "df = df[df[\"Tiempo de fermentación\"] <= 24]\n",
    "# Total de filas en el DataFrame\n",
    "n = len(df)\n",
    "\n",
    "# División del conjunto de datos en 80% entrenamiento y 20% prueba\n",
    "np.random.seed(4)\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, random_state=4)\n",
    "\n",
    "features = [\"Variedad\", \"Altura\", \"Cantidad (L)\", \"Temperatura\", \"pH\", \"Puntaje en taza\"]\n",
    "target = \"Tiempo de fermentación\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Convertir columnas de texto a tipo 'category'\n",
    "fct_df_train = df_train.copy()\n",
    "fct_df_test = df_test.copy()\n",
    "\n",
    "for col in fct_df_train.select_dtypes(include=\"object\").columns:\n",
    "    fct_df_train[col] = fct_df_train[col].astype(\"category\")\n",
    "    fct_df_test[col] = fct_df_test[col].astype(\"category\")\n",
    "\n",
    "# Definir variables independientes (X) y dependiente (y)\n",
    "features = [\"Variedad\", \"Altura\", \"Cantidad (L)\", \"Temperatura\", \"pH\", \"Puntaje en taza\"]\n",
    "target = \"Tiempo de fermentación\"\n",
    "\n",
    "# Convertir variables categóricas a variables dummy para el modelo\n",
    "X_train = pd.get_dummies(fct_df_train[features], drop_first=True)\n",
    "X_test = pd.get_dummies(fct_df_test[features], drop_first=True)\n",
    "y_train = fct_df_train[target]\n",
    "y_test = fct_df_test[target]\n",
    "\n",
    "# Asegurar que las columnas de test coincidan con las de train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reproducibilidad\n",
    "np.random.seed(28)\n",
    "\n",
    "# Concatenar los datasets de entrenamiento y prueba como en R\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "# Proporción de entrenamiento\n",
    "train_fraction = len(X_train) \/ len(X_full)\n",
    "\n",
    "# Entrenar el modelo Gradient Boosting con 10,000 árboles\n",
    "gbm_model = GradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=10000,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    subsample=0.5,\n",
    "    random_state=28\n",
    ")\n",
    "\n",
    "# Entrenar con los datos completos\n",
    "gbm_model.fit(X_full, y_full)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": gbm_model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "# Calcular error en cada iteración del modelo\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbm_model.staged_predict(X_test)]\n",
    "best_iter = np.argmin(errors) + 1  # Encontrar la mejor cantidad de árboles\n",
    "\n",
    "print(f\"Best iteration based on test data: {best_iter}\")\n",
    "\n",
    "# Generar predicciones usando la mejor cantidad de árboles\n",
    "gbm_prediction = list(gbm_model.staged_predict(X_test))[best_iter - 1]\n",
    "\n",
    "# Calcular RMSE\n",
    "rmse_gbm = np.sqrt(mean_squared_error(y_test, gbm_prediction))\n",
    "\n",
    "# Calcular R²\n",
    "rsq_gbm = np.corrcoef(y_test, gbm_prediction)[0, 1] ** 2\n",
    "\n",
    "print(f\"The GBM model is on average {round(rmse_gbm, 2)} total points off when predicting new values. \\nIts R squared value is {round(rsq_gbm, 2)}.\")\n",
    "\n",
    "# Graficar valores reales vs predichos\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, gbm_prediction, alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Gradient Boosted Tree Model: Actual vs Predicted Values\")\n",
    "plt.show()"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"rOjWYgbuwcRW5EGZQQ8U7x",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Guardar modelo onnx"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"bZ4Q9NlNwVPbcEf0ezf5Ez",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import onnx\n",
    "import pickle\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Guardar las columnas para usar en la predicción\n",
    "with open(\"columnas_Xx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train.columns.tolist(), f)\n",
    "\n",
    "# Definir forma de entrada (número de features)\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "\n",
    "# Convertir el modelo\n",
    "onnx_model = convert_sklearn(gbm_model, initial_types=initial_type, target_opset=8)\n",
    "\n",
    "# Guardar el modelo en formato ONNX\n",
    "onnx.save_model(onnx_model, \"modelo_TimeFx.onnx\")\n",
    "\n",
    "print(\"✅ Modelo exportado a ONNX correctamente.\")\n",
    "print(\"Archivo: modelo_TimeFx.onnx\")\n",
    "print(f\"Columnas guardadas en columnas_Xx.pkl ({len(X_train.columns)} features)\")"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"zQLIp2IW1p2VJvY5QSU9ea",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default_3_11",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}