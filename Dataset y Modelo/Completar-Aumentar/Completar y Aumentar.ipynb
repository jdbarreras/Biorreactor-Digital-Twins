{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# KNN"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"w1WPx900ot7AabovWfaU8g",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo Excel en un DataFrame\n",
    "df = pd.read_excel('DATA2.xlsx')\n",
    "'''\n",
    "# Drop the rows with the specified IDs\n",
    "ids_to_drop = [36, 44, 53]\n",
    "df = df[~df['ID'].isin(ids_to_drop)]  # Assuming the ID column is named 'ID'\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(df)\n",
    "'''\n",
    "# Mostrar las primeras filas del DataFrame para verificar la carga\n",
    "print(\"DataFrame original:\")\n",
    "print(df.head())\n",
    "\n",
    "df = df.drop(columns=['Variedad', 'Método', 'Fuente'], axis=1)\n",
    "\n",
    "# Seleccionar solo las columnas numéricas para la imputación\n",
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Crear el imputador KNN\n",
    "knn_imputer = KNNImputer(n_neighbors=5)  # Ajusta el número de vecinos según sea necesario\n",
    "\n",
    "# Aplicar la imputación\n",
    "df_imputed_numeric = pd.DataFrame(knn_imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "\n",
    "# Combinar las columnas imputadas con las no numéricas (si las hay)\n",
    "df_imputed = df.copy()\n",
    "df_imputed[df_numeric.columns] = df_imputed_numeric\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame imputado\n",
    "print(\"\\nDataFrame imputado con KNN:\")\n",
    "print(df_imputed.head())\n",
    "\n",
    "# Guardar el DataFrame imputado en un nuevo archivo Excel\n",
    "df_imputed.to_excel('knn.xlsx', index=False)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"FnqQ99qss4IUtTYmEnj8ns",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# GAN"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"w8VNDiJhTVzUJofZiE9E1W",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "### Variedad String -> Int\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"knn.xlsx\")\n",
    "\n",
    "# Crear un diccionario para mapear cada variedad a un número único\n",
    "variedad_a_numero = {variedad: i for i, variedad in enumerate(df['Variedad'].unique())}\n",
    "\n",
    "# Aplicar el mapeo a la columna 'Variedad'\n",
    "df['Variedad'] = df['Variedad'].map(variedad_a_numero)\n",
    "\n",
    "print(\"Enteros asignados a cada variedad:\")\n",
    "for variedad, numero in variedad_a_numero.items():\n",
    "    print(f\"{variedad}: {numero}\")\n",
    "\n",
    "conteo_variedades = df['Variedad'].value_counts()\n",
    "print(\"\\nConteo de cada variedad:\")\n",
    "print(conteo_variedades)\n",
    "\n",
    "print(\"\\nDataFrame con la columna numérica:\")\n",
    "print(df)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"oXmqVKVhFzUsq120ClyWyA",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and Normalize Data\n",
    "real_data = df\n",
    "scaler = MinMaxScaler()\n",
    "real_data_scaled = scaler.fit_transform(real_data.iloc[:, 1:])  # Drop ID column\n",
    "\n",
    "# Convert to Torch Tensor\n",
    "real_tensor = torch.tensor(real_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Extract condition column (Puntaje en Taza)\n",
    "puntaje_index = real_data.columns.get_loc(\"Puntaje en taza\") - 1  # Adjust for dropped ID column\n",
    "condition_data = real_tensor[:, puntaje_index].unsqueeze(1)  # Keep as 2D tensor\n",
    "\n",
    "# Define Dataset and DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(TensorDataset(real_tensor, condition_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator (With Conditional Input)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, condition):\n",
    "        x = torch.cat((z, condition), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator (With Conditional Input)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        x = torch.cat((x, condition), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize Models\n",
    "input_dim = 10  # Latent space size\n",
    "condition_dim = 1  # Only conditioning on \"Puntaje en Taza\"\n",
    "output_dim = real_data_scaled.shape[1]\n",
    "generator = Generator(input_dim, condition_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim, condition_dim)\n",
    "\n",
    "# Optimizers & Loss Function\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    for real_samples, conditions in dataloader:\n",
    "        batch_size = real_samples.size(0)\n",
    "        \n",
    "        # === Train Discriminator ===\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.full((batch_size, 1), 0.9)  # Label Smoothing\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        real_predictions = discriminator(real_samples, conditions)\n",
    "        loss_real = loss_function(real_predictions, real_labels)\n",
    "        \n",
    "        z = torch.randn(batch_size, input_dim)\n",
    "        fake_samples = generator(z, conditions)\n",
    "        fake_predictions = discriminator(fake_samples.detach(), conditions)\n",
    "        loss_fake = loss_function(fake_predictions, fake_labels)\n",
    "        \n",
    "        loss_D = (loss_real + loss_fake) \/ 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_predictions = discriminator(fake_samples, conditions)\n",
    "        loss_G = loss_function(fake_predictions, real_labels)  # Wants fake to be classified as real\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}\/{num_epochs} | Loss_D: {loss_D.item():.4f} | Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "# Generate Synthetic Data\n",
    "num_samples = 10000\n",
    "z = torch.randn(num_samples, input_dim)\n",
    "\n",
    "# Generate random conditions for \"Puntaje en Taza\"\n",
    "min_puntaje, max_puntaje = real_data[\"Puntaje en taza\"].min(), real_data[\"Puntaje en taza\"].max()\n",
    "random_conditions = np.random.uniform(min_puntaje, max_puntaje, (num_samples, 1))\n",
    "random_conditions_scaled = scaler.transform(np.concatenate([np.zeros((num_samples, real_data_scaled.shape[1] - 1)), random_conditions], axis=1))[:, puntaje_index]\n",
    "conditions = torch.tensor(random_conditions_scaled.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "#######\n",
    "# Identificar el índice de la columna \"Variedad\"\n",
    "variedad_index = real_data.columns.get_loc(\"Variedad\") - 1  # Ajuste por la columna ID eliminada\n",
    "#######\n",
    "\n",
    "synthetic_data = generator(z, conditions).detach().numpy()\n",
    "synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "\n",
    "######\n",
    "# Redondear solo la columna \"Variedad\"\n",
    "synthetic_data[:, variedad_index] = np.round(synthetic_data[:, variedad_index])\n",
    "# Asegurar que sean enteros\n",
    "synthetic_data[:, variedad_index] = synthetic_data[:, variedad_index].astype(int)\n",
    "######\n",
    "\n",
    "# Save Synthetic Data\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=real_data.columns[1:])\n",
    "synthetic_df.to_csv(\"synthetic_data.csv\", index=False)\n",
    "print(\"Synthetic data generated and saved!\")\n",
    "\n",
    "# Concatenate original dataset with synthetic dataset\n",
    "combined_data = pd.concat([real_data.iloc[:, 1:], synthetic_df], ignore_index=True)\n",
    "\n",
    "# Save to Excel\n",
    "combined_data.to_excel(\"combined_data.xlsx\", index=False)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"DF40XZXgnDpRCjIpm6t7A3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Statistical Similarity Check"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"fd3fsvIT77Nny0sx6UhSoy",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#### Statistical Similarity Check (Compare Real & Synthetic Data)\n",
    "###### Ensures synthetic data follows the same distribution as the real data.\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numerical columns\n",
    "real_data = real_data.iloc[:, 1:]  # Exclude ID column if present\n",
    "\n",
    "synthetic_data = synthetic_df  # Generated data\n",
    "\n",
    "# Compare distributions for each column\n",
    "for col in real_data.columns:\n",
    "    real_vals = real_data[col]\n",
    "    synthetic_vals = synthetic_data[col]\n",
    "    \n",
    "    # KS test (p > 0.05 means similar distributions)\n",
    "    ks_stat, p_value = ks_2samp(real_vals, synthetic_vals)\n",
    "    \n",
    "    print(f\"Column: {col} | KS Test p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    sns.kdeplot(real_vals, label=\"Real\", fill=True)\n",
    "    sns.kdeplot(synthetic_vals, label=\"Synthetic\", fill=True, linestyle=\"dashed\")\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "## If p-value > 0.05, real & synthetic data follow a similar distribution.\n",
    "## If p-value < 0.05, synthetic data does not match real data well."
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"wdGccmmvnodDZMOy5fNrOD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Limpieza del dataset"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"pBKpRKd8kVw74Jzvq4ZlqA",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "##DATASET CON OUTLIERS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel(\"dataset_SIN_out.xlsx\")\n",
    "\n",
    "# Primeras filas del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Información general del dataset (tipos de datos, valores no nulos)\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\nValores faltantes por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# (Opcional) Visualizaciones básicas para entender distribuciones y relaciones\n",
    "# Ejemplo: Histogramas para variables numéricas\n",
    "data.hist(figsize=(12, 10)) # Ajusta el tamaño según el número de variables\n",
    "plt.show()\n",
    "\n",
    "# Ejemplo: Matriz de correlación para ver relaciones lineales entre variables numéricas\n",
    "correlation_matrix = data.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"wBBKc5R0O7iPBGzSEQW8yD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset (reemplaza con tu archivo)\n",
    "df = pd.read_excel(\"dataset_SIN_out.xlsx\")\n",
    "\n",
    "# Redondear todos los valores numéricos a 2 decimales\n",
    "df = df.round(2)\n",
    "\n",
    "# Lista de columnas a redondear\n",
    "columnas_a_redondear1 = [\"Cantidad (L)\", \"Temperatura\"]  # Redondear a 1 decimal\n",
    "columnas_a_redondear0 = [\"Altura\", \"Tiempo de fermentación\"] # Redondear a 0 decimales\n",
    "# Aplicar el redondeo solo a esas columnas\n",
    "df[columnas_a_redondear1] = df[columnas_a_redondear1].round(1)\n",
    "df[columnas_a_redondear0] = df[columnas_a_redondear0].round(0)\n",
    "# Aproximar \"Puntaje en taza\" al múltiplo de 0.25 más cercano\n",
    "df[\"Puntaje en taza\"] = df[\"Puntaje en taza\"].apply(lambda x: round(x * 4) \/ 4)\n",
    "\n",
    "# Guardar el dataset limpio (opcional)\n",
    "df.to_csv(\"sin_outliers_rounded.csv\", index=False)\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "print(df.head())\n",
    "print(df.tail())"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"iwDA1xlOcOcaWER4n4CK5g",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default_3_11",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}